{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T04:19:22.080629Z",
     "iopub.status.busy": "2025-03-29T04:19:22.080273Z",
     "iopub.status.idle": "2025-03-29T04:19:27.690844Z",
     "shell.execute_reply": "2025-03-29T04:19:27.690149Z",
     "shell.execute_reply.started": "2025-03-29T04:19:22.080598Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_setup.py\n",
    "\"\"\"\n",
    "load data and preprocess\n",
    "create dataloader\n",
    "visualize data\n",
    "\"\"\"\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "    \n",
    "# create dataloader\n",
    "def create_dataLoader(train_dir, test_dir, transforms, batch_size, num_workers=NUM_WORKERS):\n",
    "    \"\"\"\n",
    "    create train and test dataloader\n",
    "\n",
    "    Arg:\n",
    "        train_dir: train data directory\n",
    "        test_dir: test data directory\n",
    "        transforms: torchvision transform to perform train and test data\n",
    "        batch_size:number of sample per batch in DataLoader.\n",
    "        num_workers: number of workers per DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_dataloader, test_dataloader, class_names)\n",
    "        class_names is a list of target classes\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # load data\n",
    "    train_data = datasets.ImageFolder(\n",
    "        root=train_dir,\n",
    "        transform=transforms\n",
    "        )\n",
    "    \n",
    "    test_data = datasets.ImageFolder(\n",
    "        root=test_dir,\n",
    "        transform=transforms\n",
    "    )\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # turn dataset to dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return train_dataloader, test_dataloader, class_names\n",
    "\n",
    "def vizData():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:02:08.138958Z",
     "iopub.status.busy": "2025-03-29T05:02:08.138588Z",
     "iopub.status.idle": "2025-03-29T05:02:08.151892Z",
     "shell.execute_reply": "2025-03-29T05:02:08.150968Z",
     "shell.execute_reply.started": "2025-03-29T05:02:08.138928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\"\"\"\n",
    "function for train and evaluate model\n",
    "\"\"\"\n",
    "from typing import Tuple, List, Dict\n",
    "import torch\n",
    "import torch.types\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_step(\n",
    "        model: torch.nn.Module,\n",
    "        train_dataloader: torch.utils.data.DataLoader,\n",
    "        loss_fn: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        device: torch.device) -> Tuple[float,float]:\n",
    "    \"\"\"\n",
    "    trains a pytorch model for a single epoch\n",
    "\n",
    "    train mode -> forward -> loss -> backward -> gd\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0,0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward\n",
    "        train_logits = model(X)\n",
    "\n",
    "        # loss and acc\n",
    "        loss = loss_fn(train_logits, y)\n",
    "        train_loss += loss.item()\n",
    "        train_label = train_logits.argmax(dim=1)\n",
    "        train_acc += (train_label == y).sum().item()/len(y)\n",
    "\n",
    "        # zero grad\n",
    "        optimizer.zero_grad()\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # gd\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def test_step(\n",
    "        model:torch.nn.Module,\n",
    "        test_dataloader:torch.utils.data.DataLoader,\n",
    "        loss_fn:torch.nn.Module,\n",
    "        device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    \"\"\"\n",
    "    test a pytorch model for a single epoch\n",
    "\n",
    "    eval mode -> forward -> loss\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy).\n",
    "    \"\"\"\n",
    "    # turn off gradient tracking\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0,0\n",
    "    with torch.inference_mode():        \n",
    "        for batch, (X, y) in enumerate(test_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            test_logits = model(X)\n",
    "            # test loss\n",
    "            \n",
    "            loss = loss_fn(test_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            test_label = test_logits.argmax(dim=1)\n",
    "            # test acc\n",
    "            test_acc += (test_label==y).sum().item()/len(y)\n",
    "        \n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train(\n",
    "        model:torch.nn.Module,\n",
    "        train_dataloader:torch.utils.data.DataLoader,\n",
    "        test_dataloader:torch.utils.data.DataLoader,\n",
    "        loss_fn:torch.nn.Module,\n",
    "        optimizer:torch.optim.Optimizer,\n",
    "        epochs:int,\n",
    "        device:torch.device) -> Dict[str, List[float]]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]}\n",
    "    \"\"\"\n",
    "\n",
    "    # result dictionary\n",
    "    result = {\n",
    "        'train_loss':[],\n",
    "        'train_acc':[],\n",
    "        'test_loss':[],\n",
    "        'test_acc':[]}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # train\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "\n",
    "        # test\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        # print result\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | train loss: {train_loss:.4f}, train acc: {train_acc*100:.2f}% | test loss: {test_loss:.4f}, test acc: {test_acc*100:.2f}%')\n",
    "        \n",
    "        # save result\n",
    "        result['train_loss'].append(train_loss)\n",
    "        result['train_acc'].append(train_acc)\n",
    "        result['test_loss'].append(test_loss)\n",
    "        result['test_acc'].append(test_acc)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T04:19:40.324248Z",
     "iopub.status.busy": "2025-03-29T04:19:40.323928Z",
     "iopub.status.idle": "2025-03-29T04:19:40.975722Z",
     "shell.execute_reply": "2025-03-29T04:19:40.974998Z",
     "shell.execute_reply.started": "2025-03-29T04:19:40.324221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ResNet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ResNet.py\n",
    "\"\"\"\n",
    "paper and code replicating\n",
    "model file\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# 残差块类\n",
    "class block(nn.Module):\n",
    "    def __init__(self, in_channels, intermediate_channels, indentity_downsample=None, stride=1):\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 4 # 输出的channel数是进入时的4倍\n",
    "        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv3 = nn.Conv2d(intermediate_channels, intermediate_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(intermediate_channels*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.indentity_downsample = indentity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        if self.indentity_downsample is not None: # 第一块残差块会高宽减半，减少特征图的尺寸，后续的残差块就不会\n",
    "            identity = self.indentity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# resnet类\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes): # [残差块数量， 残差块层数， input channels, 输出类别数量] eg. resnet-50: layers-> [3,4,6,3]\n",
    "        super(ResNet, self).__init__()\n",
    "        # conv1: change image channel to 64\n",
    "        self.in_channels = 64 # 进入残差块的channel\n",
    "        self.conv1 = nn.Conv2d(image_channels, self.in_channels, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # resnet layer(conv2-5)\n",
    "        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2) # 最后的intermediateput_channel=512*4\n",
    "        # average pool and fully connect\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    # resnet layer function\n",
    "    def _make_layer(self, block, num_residual_block, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, intermediate_channels*4, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(intermediate_channels*4)\n",
    "            )\n",
    "\n",
    "        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride)) # intermediate_channel = 64\n",
    "        self.in_channels = intermediate_channels*4 # 256\n",
    "\n",
    "        for i in range(num_residual_block-1):\n",
    "            # intermediate_channel是64, 但是在最后一次conv后的intermediate_channel会x4\n",
    "            layers.append(block(self.in_channels, intermediate_channels)) # 256 -> 64 -> 256\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "def ResNet50(img_channels=3, num_classes=1000):\n",
    "    return ResNet(block, [3,4,6,3], img_channels, num_classes)\n",
    "\n",
    "def ResNet101(img_channels=3, num_classes=1000):\n",
    "    return ResNet(block, [3,4,23,3], img_channels, num_classes)\n",
    "\n",
    "def ResNet152(img_channels=3, num_classes=1000):\n",
    "    return ResNet(block, [3,8,36,3], img_channels, num_classes)\n",
    "\n",
    "\n",
    "def check_output_shape(num_classes):\n",
    "    net = ResNet50(num_classes=num_classes)\n",
    "    x = torch.rand(2,3,224,224)\n",
    "    y = net(x)\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T06:38:14.051670Z",
     "iopub.status.busy": "2025-03-29T06:38:14.051332Z",
     "iopub.status.idle": "2025-03-29T06:38:14.058927Z",
     "shell.execute_reply": "2025-03-29T06:38:14.058180Z",
     "shell.execute_reply.started": "2025-03-29T06:38:14.051645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\"\"\"\n",
    "utils funciton\n",
    "save model, load model,\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def save_model(model:torch.nn.Module,\n",
    "               dir:str,\n",
    "               model_name:str):\n",
    "    \"\"\"\n",
    "    save model to target directory\n",
    "\n",
    "    Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "    \"\"\"\n",
    "\n",
    "    # create directory\n",
    "    path_name = Path(dir)\n",
    "    path_name.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # create model path\n",
    "    assert model_name.endswith('.pth') or model_name.endswith('.pt'), 'model name must ends with .pth or .pt'\n",
    "    model_path = path_name / model_name\n",
    "\n",
    "    # save model\n",
    "    print(f'save model to {model_path}')\n",
    "    torch.save(obj=model.state_dict(), f=model_path)\n",
    "\n",
    "\n",
    "def plot_loss_curve(results: Dict[str, List[float]]):\n",
    "    results = pd.DataFrame(results)\n",
    "    train_loss = results['train_loss']\n",
    "    train_acc = results['train_acc']\n",
    "    test_loss = results['test_loss']\n",
    "    test_acc = results['test_acc']\n",
    "    plt.figure(figsize=(15,5))\n",
    "    epoch = range(len(results))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epoch, train_loss, label='train loss')\n",
    "    plt.plot(epoch, test_loss, label='test loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epoch, train_acc, label='train accuracy')\n",
    "    plt.plot(epoch, test_acc, label='test accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:02:14.755018Z",
     "iopub.status.busy": "2025-03-29T05:02:14.754722Z",
     "iopub.status.idle": "2025-03-29T06:14:40.070180Z",
     "shell.execute_reply": "2025-03-29T06:14:40.068950Z",
     "shell.execute_reply.started": "2025-03-29T05:02:14.754994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33aa70248d204113b4ca88c2ae803b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | train loss: 2.6287, train acc: 36.57% | test loss: 1.6784, test acc: 55.95%\n",
      "Epoch [2/10] | train loss: 1.3851, train acc: 63.76% | test loss: 1.3240, test acc: 64.36%\n",
      "Epoch [3/10] | train loss: 1.0038, train acc: 72.87% | test loss: 1.1774, test acc: 68.30%\n",
      "Epoch [4/10] | train loss: 0.7372, train acc: 79.55% | test loss: 1.0877, test acc: 70.97%\n",
      "Epoch [5/10] | train loss: 0.5380, train acc: 84.61% | test loss: 1.1520, test acc: 70.20%\n",
      "Epoch [6/10] | train loss: 0.3789, train acc: 88.79% | test loss: 1.1733, test acc: 71.10%\n",
      "Epoch [7/10] | train loss: 0.2707, train acc: 91.90% | test loss: 1.2840, test acc: 70.34%\n",
      "Epoch [8/10] | train loss: 0.2075, train acc: 93.63% | test loss: 1.1639, test acc: 72.43%\n",
      "Epoch [9/10] | train loss: 0.1698, train acc: 94.79% | test loss: 1.2927, test acc: 70.87%\n",
      "Epoch [10/10] | train loss: 0.1451, train acc: 95.56% | test loss: 1.3143, test acc: 71.28%\n",
      "{'train_loss': [2.628661970230373, 1.3850915690732968, 1.003831155296113, 0.7372214543960385, 0.5380141305158267, 0.3788517220700915, 0.27073415425429875, 0.20747128517895536, 0.1698107408057596, 0.14512541024895334], 'train_acc': [0.36567323297562265, 0.6376140616719661, 0.7286721751622947, 0.7954580269607843, 0.8461073090553789, 0.8878855015401431, 0.9190110376921039, 0.936337906150636, 0.947875782492051, 0.955634977808691], 'test_loss': [1.6784233913352393, 1.3239543392050146, 1.1773635965179314, 1.0877401128157296, 1.1519506207168704, 1.1733254842986964, 1.284035981755064, 1.1638797218299874, 1.2926799429938047, 1.314341829167773], 'test_acc': [0.5595175096553773, 0.6435633912655971, 0.6830204619726679, 0.7097236148247178, 0.7020480540701128, 0.711044266191325, 0.7034476195781343, 0.7242531008615567, 0.7087162990196079, 0.7128105503565062]}\n"
     ]
    }
   ],
   "source": [
    "%%writefile resnet/test.py\n",
    "\"\"\"\n",
    "main file\n",
    "1. load data\n",
    "2. model\n",
    "3. train\n",
    "4. test and eval\n",
    "5. save\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import data_setup, train, ResNet, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "LR = 0.001\n",
    "HIDDEN_UNITS = 0\n",
    "\n",
    "\n",
    "\n",
    "# data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.TrivialAugmentWide(num_magnitude_bins=31) # data augmentation\n",
    "    # 归一化\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load data\n",
    "\n",
    "train_data = datasets.Food101(\n",
    "    root='resnet/data/food101',\n",
    "    split='train',\n",
    "    transform=transform,\n",
    "    target_transform=False,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "test_data = datasets.Food101(\n",
    "    root='resnet/data/food101',\n",
    "    split='test',\n",
    "    transform=transform,\n",
    "    download=False)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "class_names = train_data.classes\n",
    "#print(class_names)\n",
    "#print(len(train_dataloader.dataset))\n",
    "\n",
    "# samll dataset\n",
    "'''\n",
    "data_file = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/data/pizza_steak_sushi.zip\"\n",
    "train_dir = 'resnet/data/pizza_steak_sushi/train'\n",
    "test_dir = 'resnet/data/pizza_steak_sushi/test'\n",
    "\n",
    "# train_dataloader, test_dataloader, class_names = data_setup.create_dataLoader(train_dir, test_dir, transform, BATCH_SIZE, NUM_WORKERS)\n",
    "'''\n",
    "\n",
    "\n",
    "# load model and pretrained model\n",
    "pretrained_resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "in_features = pretrained_resnet50.fc.in_features  \n",
    "pretrained_resnet50.fc = nn.Linear(in_features, 101)\n",
    "\n",
    "resnet50model = ResNet.ResNet50(num_classes=len(class_names)).to(device)\n",
    "resnet50model.load_state_dict(pretrained_resnet50.state_dict(), strict=False)\n",
    "\n",
    "# loss func and optimier\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=resnet50model.parameters(), lr=LR)\n",
    "\n",
    "results = data_setup.train(resnet50model, train_dataloader, test_dataloader, loss_fn, optimizer, EPOCHS, device)\n",
    "print(results)\n",
    "\n",
    "utils.plot_loss_curve(results)\n",
    "\n",
    "utils.save_model(resnet50model, 'resnet/models', 'resnet50model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
